{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71aaec87",
   "metadata": {},
   "source": [
    "# **Algoritmos de Machine Learning**\n",
    "\n",
    "# **Clasificación**\n",
    "\n",
    "Es el proceso de asignar una etiqueta a un conjunto de datos. Por ejemplo, clasificar correos electrónicos como spam o no spam.\n",
    "\n",
    "# **Algoritmos de Clasificación**\n",
    "\n",
    "- **Regresión Logística**: Utiliza una función logística para modelar la probabilidad de que una instancia pertenezca a una clase.\n",
    "\n",
    "- **Árboles de Decisión**: Utiliza un modelo en forma de árbol para tomar decisiones basadas en las características de los datos.\n",
    "\n",
    "- **Máquinas de Vectores de Soporte (SVM)**: Encuentra un hiperplano que separa las clases de manera óptima en un espacio de alta dimensión.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# Tabla de algoritmos de clasificación\n",
    "\n",
    "| Algoritmo                  | Descripción                                                                 |\n",
    "|----------------------------|-----------------------------------------------------------------------------|\n",
    "| Regresión Logística         | Modelo probabilístico que utiliza una función logística para clasificar.   |\n",
    "| Árboles de Decisión         | Modelo en forma de árbol que toma decisiones basadas en características.   |\n",
    "| Máquinas de Vectores de Soporte (SVM) | Encuentra un hiperplano óptimo para separar clases en alta dimensión. |\n",
    "| K-Vecinos Más Cercanos (KNN) | Clasifica basándose en la mayoría de los vecinos más cercanos.          |\n",
    "| Naive Bayes                | Clasificador probabilístico basado en el teorema de Bayes.                 |\n",
    "| Redes Neuronales           | Modelos inspirados en el cerebro humano que aprenden patrones complejos.   |\n",
    "| Gradient Boosting          | Combina múltiples modelos débiles para mejorar la precisión.              |\n",
    "| Random Forest              | Conjunto de árboles de decisión para mejorar la precisión y reducir el sobreajuste. |\n",
    "| LightGBM                   | Algoritmo de boosting eficiente y rápido para grandes conjuntos de datos. |\n",
    "| CatBoost                   | Algoritmo de boosting que maneja bien las variables categóricas.          |\n",
    "| XGBoost                   | Algoritmo de boosting optimizado para velocidad y rendimiento.            |\n",
    "| AdaBoost                   | Combina múltiples clasificadores débiles para formar un clasificador fuerte. |\n",
    "\n",
    "\n",
    "# **Tabla de algoritmos de regresión**\n",
    "\n",
    "| Algoritmo                  | Descripción                                                                 |\n",
    "|----------------------------|-----------------------------------------------------------------------------|\n",
    "| Regresión Lineal            | Modelo lineal que predice una variable continua a partir de variables independientes. |\n",
    "| Regresión Polinómica        | Extensión de la regresión lineal que utiliza polinomios para modelar relaciones no lineales. |\n",
    "| Regresión Ridge             | Regresión lineal con regularización L2 para evitar el sobreajuste.         |\n",
    "| Regresión Lasso             | Regresión lineal con regularización L1 que puede eliminar características irrelevantes. |\n",
    "| Regresión Elastic Net       | Combina las regularizaciones L1 y L2 para mejorar la precisión del modelo. |\n",
    "| Regresión de Árboles de Decisión | Utiliza un modelo en forma de árbol para predecir valores continuos. |\n",
    "| Regresión de Bosques Aleatorios | Conjunto de árboles de decisión para mejorar la precisión y reducir el sobreajuste. |\n",
    "| Regresión de Gradient Boosting | Combina múltiples modelos débiles para mejorar la precisión en la predicción. |\n",
    "| Regresión de XGBoost        | Algoritmo de boosting optimizado para velocidad y rendimiento en regresión. |\n",
    "| Regresión de LightGBM       | Algoritmo de boosting eficiente y rápido para grandes conjuntos de datos en regresión. |\n",
    "| Regresión de CatBoost       | Algoritmo de boosting que maneja bien las variables categóricas en regresión. |\n",
    "| Regresión de Support Vector Machines (SVM) | Utiliza un hiperplano para predecir valores continuos en un espacio de alta dimensión. |\n",
    "| Regresión de K-Vecinos Más Cercanos (KNN) | Predice valores continuos basándose en la media de los vecinos más cercanos. |\n",
    "| Regresión de Redes Neuronales | Modelos inspirados en el cerebro humano que aprenden patrones complejos para la regresión. |\n",
    "\n",
    "# **Tabla de algoritmos de clustering**\n",
    "\n",
    "| Algoritmo                  | Descripción                                                                 |\n",
    "|----------------------------|-----------------------------------------------------------------------------|\n",
    "| K-Means                    | Agrupa datos en K clusters basándose en la distancia a los centroides.    |\n",
    "| DBSCAN                     | Agrupa datos densos y encuentra ruido, no requiere número de clusters.     |\n",
    "| Hierarchical Clustering    | Crea una jerarquía de clusters, puede ser aglomerativo o divisivo.        |\n",
    "| Gaussian Mixture Models (GMM) | Modela datos como una mezcla de distribuciones gaussianas.               |\n",
    "| Mean Shift                 | Encuentra densidades de puntos para identificar clusters.                  |\n",
    "| Spectral Clustering        | Utiliza la teoría espectral para agrupar datos en un espacio de alta dimensión. |\n",
    "| Affinity Propagation       | Agrupa datos basándose en la similitud entre puntos, no requiere número de clusters. |\n",
    "| OPTICS                     | Similar a DBSCAN, pero maneja mejor la variación en la densidad de los clusters. |\n",
    "| Birch                      | Agrupa grandes conjuntos de datos en memoria limitada, construyendo un árbol de clusters. |\n",
    "| Agglomerative Clustering   | Agrupa datos en una jerarquía, fusionando clusters basados en la distancia. |\n",
    "\n",
    "# **Tabla de algoritmos de reducción de dimensionalidad**\n",
    "\n",
    "| Algoritmo                  | Descripción                                                                 |\n",
    "|----------------------------|-----------------------------------------------------------------------------|\n",
    "| PCA (Análisis de Componentes Principales) | Reduce la dimensionalidad proyectando datos en componentes ortogonales. |\n",
    "| t-SNE (t-Distributed Stochastic Neighbor Embedding) | Visualiza datos de alta dimensión en 2D o 3D preservando la estructura local. |\n",
    "| UMAP (Uniform Manifold Approximation and Projection) | Similar a t-SNE, pero más rápido y preserva mejor la estructura global. |\n",
    "| LDA (Análisis Discriminante Lineal) | Reduce la dimensionalidad maximizando la separación entre clases. |\n",
    "| ICA (Análisis de Componentes Independientes) | Descompone señales en componentes independientes, útil en procesamiento de señales. |\n",
    "| Autoencoders               | Redes neuronales que aprenden a codificar y decodificar datos, reduciendo la dimensionalidad. |\n",
    "| Isomap                     | Extensión de MDS que preserva distancias geodésicas en datos no lineales. |\n",
    "| MDS (Escalamiento Multidimensional) | Proyecta datos de alta dimensión en un espacio de menor dimensión preservando distancias. |\n",
    "| Factor Analysis            | Modelo estadístico que reduce la dimensionalidad identificando factores latentes. |\n",
    "| Random Projection         | Proyecta datos de alta dimensión en un espacio de menor dimensión utilizando matrices aleatorias. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caeb826",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
